{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# get the default brain\n",
    "env = UnityEnvironment(file_name=\"/home/shashank/deep-reinforcement-learning/p1_navigation/Banana_Linux/Banana.x86_64\")\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "env.brains\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "#print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "\n",
    "#print('Number of actions:', action_size)\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "#print('States look like:', state)\n",
    "state_size = len(state)\n",
    "#print('States have length:', state_size)\n",
    "\n",
    "env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "from dqn_agent import Agent\n",
    "agent = Agent(state_size=len(env_info.vector_observations[0]), action_size=brain.vector_action_space_size,seed=0)\n",
    "\n",
    "\n",
    "while True:\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    agent.memory.add(state,action,reward,next_state,done)\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shashank/deep-reinforcement-learning/p1_navigation/dqn_agent.py:188: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  max_weight = (BATCH_SIZE * p_min)**(-self.b)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: -0.14\n",
      "Episode 200\tAverage Score: 0.023\n",
      "Episode 300\tAverage Score: -0.04\n",
      "Episode 312\tAverage Score: -0.03<class 'dqn_agent.Experience'>\n",
      "[Experience(state=array([ 0.        ,  1.        ,  0.        ,  0.        ,  0.05654223,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.90101278,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.4941442 ,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.16459106,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.14599806,\n",
      "       -1.81299925,  3.14021111]), action=2, reward=0.0, next_state=array([1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       8.66577983e-01, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 1.51953921e-01, 0.00000000e+00, 0.00000000e+00,\n",
      "       1.00000000e+00, 0.00000000e+00, 3.44443500e-01, 1.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 5.47571540e-01,\n",
      "       1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       8.70421648e-01, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 1.26635849e-01, 1.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 1.88099191e-01, 1.75833702e-06,\n",
      "       2.30593991e+00]), done=False), Experience(state=array([ 1.        ,  0.        ,  0.        ,  0.        ,  0.26821423,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.13572699,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.22856821,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.26220596,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.15138328,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.16004652,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.13160281,\n",
      "        0.05667526, -3.05908489]), action=3, reward=0.0, next_state=array([ 0.        ,  0.        ,  1.        ,  0.        ,  0.64757317,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.19166334,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.14953615,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.28971559,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.13552597,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.32068413,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.14953659,\n",
      "        0.91185212, -1.51390207]), done=False), Experience(state=array([ 0.        ,  1.        ,  0.        ,  0.        ,  0.16632311,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.76946676,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.23725633,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.26772326,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.74435353,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.75072366,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.33799428,\n",
      "       -1.29301572,  6.71693802]), action=3, reward=0.0, next_state=array([ 0.        ,  1.        ,  0.        ,  0.        ,  0.14185913,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.51340044,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.74254888,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.16179991,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.28423062,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.24075645,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.74060357,\n",
      "       -2.35106373,  2.7145102 ]), done=False), Experience(state=array([ 0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        5.81109166e-01,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  5.73437847e-02,  0.00000000e+00,  1.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  4.70685475e-02,  0.00000000e+00,\n",
      "        1.00000000e+00,  0.00000000e+00,  0.00000000e+00,  5.35238981e-01,\n",
      "        0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        4.19640094e-02,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  9.23780054e-02,  0.00000000e+00,  1.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  4.55603413e-02,  2.38418579e-06,\n",
      "       -9.27867699e+00]), action=1, reward=0.0, next_state=array([ 0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        5.95855713e-01,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  7.72474557e-02,  0.00000000e+00,  1.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  6.34057596e-02,  0.00000000e+00,\n",
      "        1.00000000e+00,  0.00000000e+00,  0.00000000e+00,  5.48821509e-01,\n",
      "        0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        5.65294586e-02,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  1.24441825e-01,  0.00000000e+00,  1.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  6.13740534e-02,  9.53674316e-07,\n",
      "       -1.02861252e+01]), done=False), Experience(state=array([ 0.        ,  1.        ,  0.        ,  0.        ,  0.59969282,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.04930684,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.44504333,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.84793687,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.73838526,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.07281434,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.46375638,\n",
      "        0.07545686, -8.49199486]), action=1, reward=0.0, next_state=array([ 0.        ,  1.        ,  0.        ,  0.        ,  0.52222568,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.06821248,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.42886239,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.86610436,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.7549879 ,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.10834946,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.47332081,\n",
      "        0.02818918, -9.95418453]), done=False), Experience(state=array([ 0.        ,  1.        ,  0.        ,  0.        ,  0.27229947,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.3670567 ,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.41739571,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.12012856,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.22020696,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.55766475,\n",
      "       -0.47844473, -4.54204988]), action=1, reward=0.0, next_state=array([ 0.        ,  1.        ,  0.        ,  0.        ,  0.26966968,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.38228482,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.41336465,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.127243  ,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.23967591,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.57321858,\n",
      "       -0.20360279, -8.62647152]), done=False), Experience(state=array([ 0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.43560502,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.95220667,\n",
      "        0.06229866, -0.80796301]), action=1, reward=0.0, next_state=array([ 0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.89359152,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.44704109,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.96180689,\n",
      "        0.03354931, -7.44625425]), done=False), Experience(state=array([ 0.        ,  1.        ,  0.        ,  0.        ,  0.11710054,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.15685274,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.16947223,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.33996731,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.46382621,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.77282572,\n",
      "       -0.63582623, -5.7503624 ]), action=1, reward=0.0, next_state=array([ 0.        ,  1.        ,  0.        ,  0.        ,  0.11577059,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.16800919,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.16754745,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.35362262,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.45855835,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.78853995,\n",
      "       -0.23120618, -8.7982626 ]), done=False), Experience(state=array([1.        , 0.        , 0.        , 0.        , 0.07109746,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.82568967,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.09186568,\n",
      "       0.        , 0.        , 1.        , 0.        , 0.45514688,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.18439734,\n",
      "       1.        , 0.        , 0.        , 0.        , 0.82653373,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.75614846,\n",
      "       0.93914032, 6.43622112]), action=1, reward=0.0, next_state=array([ 1.        ,  0.        ,  0.        ,  0.        ,  0.06978478,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.82751828,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.09246165,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.45506391,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.18559356,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.82722563,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.75782305,\n",
      "        0.41081214, -4.25793886]), done=False), Experience(state=array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        1.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        1.00000000e+00,  0.00000000e+00,  8.91329646e-01,  0.00000000e+00,\n",
      "        0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  7.20238328e-01,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
      "        0.00000000e+00,  8.22175264e-01,  0.00000000e+00,  0.00000000e+00,\n",
      "        1.00000000e+00,  0.00000000e+00,  9.56139207e-01, -2.86102295e-06,\n",
      "        5.18845844e+00]), action=1, reward=0.0, next_state=array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        1.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        1.00000000e+00,  0.00000000e+00,  8.90344799e-01,  0.00000000e+00,\n",
      "        0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  7.22964942e-01,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
      "        0.00000000e+00,  8.24867725e-01,  0.00000000e+00,  0.00000000e+00,\n",
      "        1.00000000e+00,  0.00000000e+00,  9.59101737e-01,  2.86102295e-06,\n",
      "       -5.18845844e+00]), done=False), Experience(state=array([ 0.        ,  1.        ,  0.        ,  0.        ,  0.77303725,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.35358253,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.32156819,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.44536063,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.39761838,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.36017913,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.39387611,\n",
      "        0.11039579, -4.72017384]), action=2, reward=0.0, next_state=array([ 0.        ,  1.        ,  0.        ,  0.        ,  0.42764106,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.25192305,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.15929952,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.36217338,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.31754541,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.3750349 ,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.37494606,\n",
      "       -1.60560989, -2.93755174]), done=False), Experience(state=array([0.        , 1.        , 0.        , 0.        , 0.01392702,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.54075307,\n",
      "       0.        , 0.        , 0.        , 1.        , 0.        ,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.01830604,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.75162101,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.03674513,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.57182455,\n",
      "       0.01883965, 4.85555887]), action=1, reward=0.0, next_state=array([ 0.        ,  1.        ,  0.        ,  0.        ,  0.0139526 ,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.54332101,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.01833966,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.75519043,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.03681262,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.57454014,\n",
      "        0.00755174, -4.84406662]), done=False), Experience(state=array([ 0.        ,  0.        ,  1.        ,  0.        ,  0.45598343,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.97829109,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.41449434,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.5861724 ,\n",
      "       -0.97301608, -6.37234974]), action=3, reward=0.0, next_state=array([ 0.        ,  1.        ,  0.        ,  0.        ,  0.08403298,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.40718642,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.39126378,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.76396477,\n",
      "        1.20711541, -3.09316492]), done=False), Experience(state=array([ 0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        4.68542874e-01,  1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  3.72115105e-01,  1.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  7.11577892e-01,  0.00000000e+00,\n",
      "        0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  2.67981976e-01,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
      "        0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  1.42091453e-01,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  4.84299660e-03,\n",
      "       -6.15250969e+00]), action=1, reward=0.0, next_state=array([ 0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        4.81073409e-01,  1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  3.88189703e-01,  1.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  5.72155476e-01,  0.00000000e+00,\n",
      "        0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  2.75405526e-01,\n",
      "        0.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "        2.69652992e-01,  1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  1.58598304e-01,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  1.76858902e-03,\n",
      "       -8.96700954e+00]), done=False), Experience(state=array([ 0.        ,  1.        ,  0.        ,  0.        ,  0.41039631,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.78316379,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.58338052,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.50264221,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.49344566,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.85506219,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.79286784,\n",
      "       -1.88298392, -3.09834552]), action=2, reward=0.0, next_state=array([ 0.        ,  1.        ,  0.        ,  0.        ,  0.54499418,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.73535341,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.39473072,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.84727502,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.57863992,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.78039545,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "       -2.02213788, -1.10760999]), done=False), Experience(state=array([ 0.        ,  1.        ,  0.        ,  0.        ,  0.11555263,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.35489142,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.26762679,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.24202366,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.3450135 ,\n",
      "       -0.62091851, -6.14450455]), action=2, reward=0.0, next_state=array([ 0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.28551337,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.30993181,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.26469532,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.35895053,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.6324355 ,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.32380924,\n",
      "       -2.25035095, -3.12360358]), done=False), Experience(state=array([ 0.        ,  1.        ,  0.        ,  0.        ,  0.52298772,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.19603363,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.16883844,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.2678791 ,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.28726465,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.20599644,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.21130022,\n",
      "       -0.86218613, -3.25067186]), action=0, reward=0.0, next_state=array([ 0.        ,  1.        ,  0.        ,  0.        ,  0.50988859,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.19112363,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.16517858,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.26116961,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.28006959,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.20083688,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.20600784,\n",
      "       -0.46521497,  5.93130302]), done=False), Experience(state=array([1.        , 0.        , 0.        , 0.        , 0.56396759,\n",
      "       0.        , 0.        , 1.        , 0.        , 0.39680824,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.04986967,\n",
      "       0.        , 0.        , 0.        , 1.        , 0.        ,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.17827103,\n",
      "       0.        , 0.        , 1.        , 0.        , 0.94596499,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.69824362,\n",
      "       0.        , 5.18851089]), action=2, reward=0.0, next_state=array([0.        , 0.        , 0.        , 1.        , 0.        ,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.66905111,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.03572182,\n",
      "       0.        , 0.        , 0.        , 1.        , 0.        ,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.05101812,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.74031186,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.1327181 ,\n",
      "       1.52586734, 2.64288402]), done=False), Experience(state=array([ 0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        3.11381638e-01,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  4.43025865e-02,  0.00000000e+00,  1.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  8.17749053e-02,  0.00000000e+00,\n",
      "        1.00000000e+00,  0.00000000e+00,  0.00000000e+00,  7.95660168e-02,\n",
      "        0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        5.16702160e-02,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  5.10991812e-02,  0.00000000e+00,  1.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  4.37602699e-02, -1.49011612e-06,\n",
      "        2.99906683e+00]), action=1, reward=0.0, next_state=array([ 0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        3.49589437e-01,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  4.97386977e-02,  0.00000000e+00,  1.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  9.18090120e-02,  0.00000000e+00,\n",
      "        1.00000000e+00,  0.00000000e+00,  0.00000000e+00,  8.93290937e-02,\n",
      "        0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        5.80103621e-02,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  5.73692545e-02,  0.00000000e+00,  1.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  4.91298288e-02, -8.34465027e-07,\n",
      "       -6.15734100e+00]), done=False), Experience(state=array([0.        , 1.        , 0.        , 0.        , 0.34915423,\n",
      "       1.        , 0.        , 0.        , 0.        , 0.3197009 ,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.46306533,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.34512353,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.38163984,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.41875798,\n",
      "       0.        , 0.        , 1.        , 0.        , 0.17750858,\n",
      "       2.00666094, 3.50583839]), action=0, reward=0.0, next_state=array([0.        , 1.        , 0.        , 0.        , 0.33905834,\n",
      "       1.        , 0.        , 0.        , 0.        , 0.30579165,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.45093778,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.33514416,\n",
      "       0.        , 0.        , 1.        , 0.        , 0.26573882,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.40664947,\n",
      "       0.        , 0.        , 1.        , 0.        , 0.1671005 ,\n",
      "       0.84231377, 8.12822437]), done=False), Experience(state=array([ 0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        1.46531701e-01,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  2.70471931e-01,  0.00000000e+00,  0.00000000e+00,\n",
      "        1.00000000e+00,  0.00000000e+00,  5.50095797e-01,  0.00000000e+00,\n",
      "        1.00000000e+00,  0.00000000e+00,  0.00000000e+00,  1.47099435e-01,\n",
      "        0.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "        5.67697346e-01,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  1.81886956e-01,  0.00000000e+00,  1.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  6.89360261e-01, -1.64508820e-05,\n",
      "        5.70350599e+00]), action=1, reward=0.0, next_state=array([ 0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        1.47930831e-01,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  2.73054481e-01,  0.00000000e+00,  0.00000000e+00,\n",
      "        1.00000000e+00,  0.00000000e+00,  5.52084506e-01,  0.00000000e+00,\n",
      "        1.00000000e+00,  0.00000000e+00,  0.00000000e+00,  1.48504004e-01,\n",
      "        0.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "        5.69450736e-01,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  1.83623701e-01,  0.00000000e+00,  1.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  6.95942461e-01, -5.00679016e-06,\n",
      "       -4.85136700e+00]), done=False), Experience(state=array([ 0.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "        2.35094458e-01,  1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  9.47370887e-01,  0.00000000e+00,  1.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  2.29879737e-01,  1.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  1.45941541e-01,\n",
      "        0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        3.12524080e-01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        1.00000000e+00,  0.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  6.89739287e-01, -1.17131749e-10,\n",
      "        2.52740495e-09]), action=1, reward=0.0, next_state=array([ 0.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "        2.35092074e-01,  1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  9.47369158e-01,  0.00000000e+00,  1.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  2.29879737e-01,  1.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  1.45938963e-01,\n",
      "        0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        3.12524080e-01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        1.00000000e+00,  0.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  6.89739287e-01, -8.96517721e-11,\n",
      "        2.82843082e-09]), done=False), Experience(state=array([ 0.        ,  0.        ,  1.        ,  0.        ,  0.70089972,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.28775939,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.33431438,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.16533384,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.07770646,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.20437689,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "       -1.08175135, -1.87364161]), action=1, reward=0.0, next_state=array([ 0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.30097386,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.33907422,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.17506669,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.0879548 ,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.2143161 ,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "       -0.51639509, -7.62959528]), done=False), Experience(state=array([ 0.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "        1.50756851e-01,  1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  2.05643386e-01,  0.00000000e+00,  1.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  6.37170017e-01,  0.00000000e+00,\n",
      "        1.00000000e+00,  0.00000000e+00,  0.00000000e+00,  6.29881144e-01,\n",
      "        0.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "        2.55776525e-01,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  7.07645416e-01,  0.00000000e+00,  1.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  4.78161722e-01, -4.76837158e-07,\n",
      "        5.18851185e+00]), action=1, reward=0.0, next_state=array([ 0.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "        9.86878946e-02,  1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  2.08946839e-01,  0.00000000e+00,  1.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  6.41095042e-01,  0.00000000e+00,\n",
      "        1.00000000e+00,  0.00000000e+00,  0.00000000e+00,  6.31502986e-01,\n",
      "        0.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "        2.56659627e-01,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  7.12004483e-01,  0.00000000e+00,  1.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  4.81107175e-01,  4.76837158e-07,\n",
      "       -5.18851185e+00]), done=False), Experience(state=array([ 0.        ,  1.        ,  0.        ,  0.        ,  0.62447268,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.31230605,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.26354501,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.6124993 ,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.72962642,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.73636359,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.74388945,\n",
      "       -0.18557644, -5.90169191]), action=0, reward=0.0, next_state=array([ 0.        ,  1.        ,  0.        ,  0.        ,  0.62329727,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.31022435,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.2625291 ,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.61134642,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.7276457 ,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.73497748,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.74186999,\n",
      "       -0.0712676 ,  4.68744564]), done=False), Experience(state=array([0.        , 0.        , 1.        , 0.        , 0.4994033 ,\n",
      "       0.        , 0.        , 0.        , 1.        , 0.        ,\n",
      "       1.        , 0.        , 0.        , 0.        , 0.17253454,\n",
      "       1.        , 0.        , 0.        , 0.        , 0.76453763,\n",
      "       0.        , 0.        , 1.        , 0.        , 0.31360391,\n",
      "       0.        , 0.        , 1.        , 0.        , 0.53466994,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.96545291,\n",
      "       0.3421669 , 9.09361649]), action=1, reward=0.0, next_state=array([ 0.        ,  0.        ,  1.        ,  0.        ,  0.49879378,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.1734771 ,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.76361197,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.31319064,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.53341383,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.96449006,\n",
      "        0.07397985, -3.32004619]), done=False), Experience(state=array([ 1.        ,  0.        ,  0.        ,  0.        ,  0.9336133 ,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.83289117,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.06802621,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.09040217,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.26326898,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.18690334,\n",
      "       -1.52586758, -2.64288187]), action=1, reward=0.0, next_state=array([ 0.        ,  0.        ,  1.        ,  0.        ,  0.36361653,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.85769868,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.06564846,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.08724232,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.41729748,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.18037042,\n",
      "       -0.74614936, -8.08382416]), done=False), Experience(state=array([ 1.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        5.13938546e-01,  1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  2.58911282e-01,  0.00000000e+00,  1.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  1.22481562e-01,  0.00000000e+00,\n",
      "        0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  4.66564864e-01,\n",
      "        0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        6.09399557e-01,  0.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
      "        0.00000000e+00,  1.43643469e-01,  0.00000000e+00,  1.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  6.95061505e-01,  2.38418579e-07,\n",
      "       -5.18851042e+00]), action=0, reward=0.0, next_state=array([0.        , 0.        , 1.        , 0.        , 0.02534327,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.13548823,\n",
      "       1.        , 0.        , 0.        , 0.        , 0.23629673,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.13647132,\n",
      "       0.        , 0.        , 1.        , 0.        , 0.09011354,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.12569907,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.16904172,\n",
      "       0.        , 0.        ]), done=True), Experience(state=array([0.        , 1.        , 0.        , 0.        , 0.05635278,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.4599863 ,\n",
      "       1.        , 0.        , 0.        , 0.        , 0.7973336 ,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.25016829,\n",
      "       0.        , 0.        , 0.        , 1.        , 0.        ,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.309331  ,\n",
      "       0.        , 0.        , 1.        , 0.        , 0.35044941,\n",
      "       0.        , 6.89104605]), action=1, reward=0.0, next_state=array([ 0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        5.38101867e-02,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  4.60609794e-01,  1.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  7.96644568e-01,  0.00000000e+00,\n",
      "        1.00000000e+00,  0.00000000e+00,  0.00000000e+00,  2.50507355e-01,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  3.09750199e-01,  0.00000000e+00,  0.00000000e+00,\n",
      "        1.00000000e+00,  0.00000000e+00,  3.51123929e-01, -2.38418579e-07,\n",
      "       -3.89143038e+00]), done=False), Experience(state=array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 5.16312718e-01, 0.00000000e+00, 1.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 1.68120682e-01, 1.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 5.13899207e-01,\n",
      "       0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       1.87022895e-01, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 7.92326868e-01, 0.00000000e+00, 1.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 2.67105609e-01, 3.57627869e-07,\n",
      "       5.18850994e+00]), action=1, reward=0.0, next_state=array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  5.19616246e-01,  0.00000000e+00,  1.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  1.69196382e-01,  1.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  5.20771742e-01,\n",
      "        0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        1.88219532e-01,  1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  7.95708239e-01,  0.00000000e+00,  1.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  2.68814653e-01,  7.51018524e-06,\n",
      "       -5.18848228e+00]), done=False), Experience(state=array([0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       1.10649750e-01, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 2.13754717e-02, 0.00000000e+00, 1.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 4.35470976e-02, 0.00000000e+00,\n",
      "       1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.59186754e-02,\n",
      "       0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       2.60961000e-02, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 2.41389032e-02, 0.00000000e+00, 1.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 2.15074141e-02, 6.25848770e-06,\n",
      "       5.18846369e+00]), action=1, reward=0.0, next_state=array([ 0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        1.27749652e-01,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  2.46788599e-02,  0.00000000e+00,  1.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  5.02769053e-02,  0.00000000e+00,\n",
      "        1.00000000e+00,  0.00000000e+00,  0.00000000e+00,  4.14695852e-02,\n",
      "        0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        3.01290154e-02,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  2.78693512e-02,  0.00000000e+00,  1.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  2.48311851e-02, -6.25848770e-06,\n",
      "       -5.18846369e+00]), done=False), Experience(state=array([ 0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.60483283,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.0348251 ,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.52110815,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.03792845,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.61077285,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.60590982,\n",
      "        0.92314219, -8.28556538]), action=2, reward=0.0, next_state=array([ 0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.0465919 ,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.03978141,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.04149133,\n",
      "       -1.53298986, -3.50178862]), done=False), Experience(state=array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
      "        0.00000000e+00,  1.02728307e-01,  1.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  4.11351889e-01,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "        5.77014506e-01,  0.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
      "        0.00000000e+00,  5.31730294e-01,  1.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  5.71990371e-01,  2.38418579e-07,\n",
      "       -5.70343876e+00]), action=0, reward=0.0, next_state=array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
      "       0.00000000e+00, 1.00145668e-01, 1.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 4.12333190e-01, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
      "       5.73712945e-01, 0.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
      "       0.00000000e+00, 5.31137824e-01, 1.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 5.69463909e-01, 1.19209290e-07,\n",
      "       4.85142136e+00]), done=False), Experience(state=array([ 0.        ,  1.        ,  0.        ,  0.        ,  0.00424358,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.00828485,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.2585797 ,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.46089691,\n",
      "       -0.06622802, -0.19447756]), action=1, reward=0.0, next_state=array([ 0.        ,  1.        ,  0.        ,  0.        ,  0.00424358,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.00828485,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.25940973,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.46107212,\n",
      "       -0.06622802, -0.19447756]), done=False), Experience(state=array([0.        , 1.        , 0.        , 0.        , 0.09272604,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.03323513,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.10206804,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.04619521,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.04783656,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.0351448 ,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.03559421,\n",
      "       2.04000473, 3.53339744]), action=2, reward=0.0, next_state=array([0.        , 1.        , 0.        , 0.        , 0.03540052,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.03209567,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.33727109,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.02835359,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.11371983,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.02802628,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.04371597,\n",
      "       2.3709178 , 1.36885428]), done=False), Experience(state=array([ 0.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "        2.02234954e-01,  0.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
      "        0.00000000e+00,  1.42293498e-01,  0.00000000e+00,  1.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  6.07671499e-01,  0.00000000e+00,\n",
      "        0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  1.15903333e-01,\n",
      "        0.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "        1.38626188e-01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        1.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  1.00000000e+00,  0.00000000e+00, -6.95109367e-04,\n",
      "       -4.91948748e+00]), action=0, reward=0.0, next_state=array([ 0.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "        2.00167924e-01,  0.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
      "        0.00000000e+00,  1.39850125e-01,  0.00000000e+00,  1.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  6.08565211e-01,  0.00000000e+00,\n",
      "        0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  1.14215009e-01,\n",
      "        0.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "        1.36081919e-01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        1.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  1.00000000e+00,  0.00000000e+00, -2.94208527e-04,\n",
      "        4.78336763e+00]), done=False), Experience(state=array([ 0.        ,  0.        ,  1.        ,  0.        ,  0.809793  ,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.29933789,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.72147512,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.90337747,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.56170309,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "       -1.27917933, -2.21560669]), action=0, reward=0.0, next_state=array([ 0.        ,  0.        ,  1.        ,  0.        ,  0.80718148,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.29275882,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.49614257,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.89903265,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.55129886,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "       -0.6481784 ,  6.00514174]), done=False), Experience(state=array([ 0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        2.51020551e-01,  1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  1.28889382e-01,  0.00000000e+00,  1.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  7.63139427e-01,  0.00000000e+00,\n",
      "        1.00000000e+00,  0.00000000e+00,  0.00000000e+00,  2.79243559e-01,\n",
      "        1.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        1.21523820e-01,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  3.98815304e-01,  0.00000000e+00,  1.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  4.80331600e-01, -9.53674316e-07,\n",
      "        2.99905038e+00]), action=3, reward=0.0, next_state=array([ 0.        ,  1.        ,  0.        ,  0.        ,  0.28530854,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.33063975,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.50992948,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.250146  ,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.47722232,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.26729015,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.12106635,\n",
      "       -0.85485739,  1.4806571 ]), done=False), Experience(state=array([ 0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.41902757,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.89928871,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.41455635,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.5725981 ,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.45166075,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.10950105,\n",
      "        0.09850868, -5.18706036]), action=0, reward=0.0, next_state=array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 4.15724844e-01, 0.00000000e+00, 1.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 8.99445295e-01, 1.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 4.11413610e-01,\n",
      "       0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       5.68084955e-01, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 4.48100775e-01, 0.00000000e+00, 0.00000000e+00,\n",
      "       1.00000000e+00, 0.00000000e+00, 1.07868761e-01, 1.86264515e-06,\n",
      "       5.18845558e+00]), done=False), Experience(state=array([ 0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        8.51968467e-01,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  7.53607810e-01,  1.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  1.58031210e-02,  1.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  2.32465997e-01,\n",
      "        1.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        3.17118913e-02,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  6.62712276e-01,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  4.76837158e-07,\n",
      "       -8.56024075e+00]), action=2, reward=0.0, next_state=array([ 1.        ,  0.        ,  0.        ,  0.        ,  0.23367903,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.38528714,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.00893851,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.68473679,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.00945302,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.86340135,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.01654774,\n",
      "       -1.97655511, -3.42349911]), done=False), 0, Experience(state=array([ 0.        ,  0.        ,  1.        ,  0.        ,  0.02910226,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.09054079,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.84808671,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.02830886,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.37457356,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.03628093,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "       -0.94019693,  7.39494038]), action=2, reward=0.0, next_state=array([0.        , 0.        , 1.        , 0.        , 0.02410019,\n",
      "       0.        , 0.        , 1.        , 0.        , 0.77409089,\n",
      "       1.        , 0.        , 0.        , 0.        , 0.08958843,\n",
      "       1.        , 0.        , 0.        , 0.        , 0.07579586,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.82381266,\n",
      "       0.        , 0.        , 1.        , 0.        , 0.64017224,\n",
      "       0.        , 0.        , 1.        , 0.        , 0.35734668,\n",
      "       1.55219412, 3.70079684]), done=False), Experience(state=array([ 0.        ,  0.        ,  1.        ,  0.        ,  0.0977162 ,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.28789505,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.18636964,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.11251617,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.17857857,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.24488156,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.20900556,\n",
      "        2.24508071, -3.07357407]), action=0, reward=0.0, next_state=array([0.        , 0.        , 1.        , 0.        , 0.08978494,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.28695068,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.18575829,\n",
      "       1.        , 0.        , 0.        , 0.        , 0.74824274,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.17799275,\n",
      "       1.        , 0.        , 0.        , 0.        , 0.23807535,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.20831995,\n",
      "       1.48181248, 6.13472652]), done=False), Experience(state=array([1.        , 0.        , 0.        , 0.        , 0.25089073,\n",
      "       0.        , 0.        , 1.        , 0.        , 0.27216357,\n",
      "       1.        , 0.        , 0.        , 0.        , 0.36386979,\n",
      "       0.        , 0.        , 1.        , 0.        , 0.98476446,\n",
      "       0.        , 0.        , 0.        , 1.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 1.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 1.        , 0.        ,\n",
      "       0.69416261, 6.10116863]), action=3, reward=0.0, next_state=array([ 0.        ,  0.        ,  1.        ,  0.        ,  0.37227586,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.76829374,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.2365711 ,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "       -1.52429032,  3.50411606]), done=False), Experience(state=array([ 1.        ,  0.        ,  0.        ,  0.        ,  0.23393106,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.84813166,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.13219538,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.32149509,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.15591443,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.56521475,\n",
      "        1.40487671, -6.17608309]), action=0, reward=0.0, next_state=array([1.        , 0.        , 0.        , 0.        , 0.23319508,\n",
      "       1.        , 0.        , 0.        , 0.        , 0.84568143,\n",
      "       0.        , 0.        , 1.        , 0.        , 0.1333383 ,\n",
      "       0.        , 0.        , 0.        , 1.        , 0.        ,\n",
      "       1.        , 0.        , 0.        , 0.        , 0.31837234,\n",
      "       1.        , 0.        , 0.        , 0.        , 0.15374678,\n",
      "       1.        , 0.        , 0.        , 0.        , 0.56612772,\n",
      "       0.65797472, 4.43995094]), done=False), Experience(state=array([ 0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.1020566 ,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.41646782,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.59787172,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.29160318,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.11987826,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.26213086,\n",
      "        0.90964317, -7.51184464]), action=1, reward=0.0, next_state=array([ 1.        ,  0.        ,  0.        ,  0.        ,  0.4812485 ,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.12001997,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.44365588,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.63690221,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.31063974,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.35926366,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.27924338,\n",
      "        0.3370769 , -9.5420351 ]), done=False), Experience(state=array([0.        , 0.        , 1.        , 0.        , 0.32954437,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.3402887 ,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.19020951,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.61736757,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.18947493,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.61976099,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.2320639 ,\n",
      "       0.3675983 , 5.08442545]), action=2, reward=0.0, next_state=array([0.        , 1.        , 0.        , 0.        , 0.60220927,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.2041972 ,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.2487727 ,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.58877492,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.191304  ,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.26677647,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.18330729,\n",
      "       1.67128265, 2.46521139]), done=False), Experience(state=array([0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       1.40635418e-02, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 4.11735103e-03, 0.00000000e+00, 1.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 1.05229793e-02, 0.00000000e+00,\n",
      "       1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 6.14488823e-03,\n",
      "       0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       5.53282024e-03, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 4.46680607e-03, 0.00000000e+00, 1.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 4.29957081e-03, 7.34420884e-30,\n",
      "       3.84934480e-31]), action=1, reward=0.0, next_state=array([ 0.        ,  1.        ,  0.        ,  0.        ,  0.04605562,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.0134836 ,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.0344609 ,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.02012342,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.01811901,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.014628  ,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.01408034,\n",
      "        0.        , -6.89104414]), done=False), Experience(state=array([ 1.        ,  0.        ,  0.        ,  0.        ,  0.32667863,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.54797173,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.47752652,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.50624001,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.67275673,\n",
      "       -1.08506548, -6.16094685]), action=3, reward=0.0, next_state=array([ 0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.52710623,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.34321523,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.74320239,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.79070717,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.77842313,\n",
      "        1.09147716, -2.99694157]), done=False), Experience(state=array([0.        , 1.        , 0.        , 0.        , 0.45640063,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.32936707,\n",
      "       0.        , 0.        , 1.        , 0.        , 0.3341291 ,\n",
      "       0.        , 0.        , 1.        , 0.        , 0.20668799,\n",
      "       1.        , 0.        , 0.        , 0.        , 0.35309979,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.30557191,\n",
      "       1.        , 0.        , 0.        , 0.        , 0.18538728,\n",
      "       0.46048415, 3.97323322]), action=1, reward=0.0, next_state=array([ 0.        ,  1.        ,  0.        ,  0.        ,  0.46127152,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.33288217,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.33301952,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.2081953 ,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.35696641,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.30883306,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.18887801,\n",
      "        0.16023028, -5.32561207]), done=False), Experience(state=array([0.        , 0.        , 1.        , 0.        , 0.06828205,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.71795785,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.80221421,\n",
      "       0.        , 0.        , 1.        , 0.        , 0.06734727,\n",
      "       0.        , 0.        , 1.        , 0.        , 0.13080318,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.82220513,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.71355158,\n",
      "       0.46109486, 8.83398247]), action=0, reward=0.0, next_state=array([ 0.        ,  0.        ,  1.        ,  0.        ,  0.05657816,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.69870269,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.80713153,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.0558036 ,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.115514  ,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.80015421,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.12281322,\n",
      "        0.17272472, 10.09878922]), done=False), Experience(state=array([ 1.        ,  0.        ,  0.        ,  0.        ,  0.6581291 ,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.08820105,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.03450987,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.86017919,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.03716914,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.658539  ,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.05053197,\n",
      "       -1.44017196, -1.21307349]), action=3, reward=0.0, next_state=array([ 1.        ,  0.        ,  0.        ,  0.        ,  0.5103786 ,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.51101351,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.03752364,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.6894471 ,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.05430566,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.14862901,\n",
      "       -0.22060272, -0.60966718]), done=False), Experience(state=array([ 0.        ,  1.        ,  0.        ,  0.        ,  0.02906085,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.94457662,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.32707551,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.03905607,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.61771578,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.08333301,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.76988393,\n",
      "       -1.47085083,  5.79083776]), action=0, reward=0.0, next_state=array([ 0.        ,  1.        ,  0.        ,  0.        ,  0.03113479,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.92897379,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.04184332,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.60120136,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.08928009,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.75380945,\n",
      "       -0.53562331,  8.81932735]), done=False), Experience(state=array([ 1.        ,  0.        ,  0.        ,  0.        ,  0.2193369 ,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.28098825,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.57315558,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.95065123,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "       -0.08421659, -2.35290384]), action=1, reward=0.0, next_state=array([ 1.        ,  0.        ,  0.        ,  0.        ,  0.22163533,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.2935884 ,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.55965769,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.66515058,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.94651896,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.74587065,\n",
      "       -0.04066563, -7.89969444]), done=False), Experience(state=array([ 1.        ,  0.        ,  0.        ,  0.        ,  0.70533592,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.97810018,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.24560368,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.06582283,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.06117061,\n",
      "       -2.05837655, -2.64890313]), action=2, reward=0.0, next_state=array([ 0.        ,  0.        ,  1.        ,  0.        ,  0.3766793 ,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.05456114,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.23115507,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.28538808,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.2356742 ,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.07767217,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.2971687 ,\n",
      "       -1.89673769, -0.77212989]), done=False), Experience(state=array([ 0.        ,  0.        ,  1.        ,  0.        ,  0.7359429 ,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.22087663,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.12346125,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.56889796,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.12298474,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.53478199,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.15062885,\n",
      "       -0.01183569,  3.48565531]), action=1, reward=0.0, next_state=array([ 0.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "        7.42213011e-01,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  2.25347534e-01,  0.00000000e+00,  1.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  1.25960305e-01,  0.00000000e+00,\n",
      "        0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  5.80917180e-01,\n",
      "        0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        1.25474140e-01,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  5.45606792e-01,  0.00000000e+00,  1.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  1.53677821e-01, -5.38706779e-03,\n",
      "       -5.69461012e+00]), done=False), Experience(state=array([ 0.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "        4.28404570e-01,  1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  3.81217659e-01,  0.00000000e+00,  1.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  4.16448593e-01,  0.00000000e+00,\n",
      "        0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  1.90480888e-01,\n",
      "        0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        6.70343399e-01,  1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  6.18291795e-01,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  3.05891037e-04,\n",
      "       -4.74771404e+00]), action=2, reward=0.0, next_state=array([ 0.        ,  0.        ,  1.        ,  0.        ,  0.19408491,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.31761396,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.34419483,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.39257839,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.4176769 ,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.58415484,\n",
      "       -1.68614936, -2.92093444]), done=False), Experience(state=array([ 0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        2.01383874e-01,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  1.57121405e-01,  1.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  3.39487940e-01,  0.00000000e+00,\n",
      "        1.00000000e+00,  0.00000000e+00,  0.00000000e+00,  1.51539013e-01,\n",
      "        0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        4.16352868e-01,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  1.42944172e-01,  0.00000000e+00,  1.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  2.01384753e-01, -1.72585690e+00,\n",
      "        3.11434269e-06]), action=3, reward=0.0, next_state=array([ 0.        ,  1.        ,  0.        ,  0.        ,  0.55340236,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.14377958,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.33891904,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.22282952,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.18697713,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.15803938,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.1482853 ,\n",
      "       -0.43207932, -0.24945983]), done=False), Experience(state=array([ 1.        ,  0.        ,  0.        ,  0.        ,  0.40822631,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.81280154,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.24561216,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.08565387,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.52026862,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.32838768,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.151917  ,\n",
      "        1.52404952, -0.87991214]), action=1, reward=0.0, next_state=array([ 0.        ,  0.        ,  1.        ,  0.        ,  0.08309548,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.82388341,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.25966349,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.10058665,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.52312708,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.33249864,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.16377401,\n",
      "        0.83625507, -7.52835321]), done=False), Experience(state=array([ 0.00000000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "        3.55396450e-01,  1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  3.57110679e-01,  0.00000000e+00,  1.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  3.64886642e-01,  1.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  6.06045127e-01,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
      "        0.00000000e+00,  1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  3.48519772e-01,  0.00000000e+00,  0.00000000e+00,\n",
      "        1.00000000e+00,  0.00000000e+00,  3.88692886e-01,  7.24792480e-05,\n",
      "       -6.89111376e+00]), action=0, reward=0.0, next_state=array([0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
      "       3.54313940e-01, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 3.56487095e-01, 0.00000000e+00, 1.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 3.66716474e-01, 1.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 6.05416596e-01,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
      "       0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 3.47488284e-01, 0.00000000e+00, 0.00000000e+00,\n",
      "       1.00000000e+00, 0.00000000e+00, 3.87620062e-01, 2.58684158e-05,\n",
      "       3.89137340e+00]), done=False), Experience(state=array([ 0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.48959228,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.1797477 ,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.25028014,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.30368632,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.54855794,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.35543138,\n",
      "       -1.04162717,  6.52135658]), action=1, reward=0.0, next_state=array([ 0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.48927811,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.17424814,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.25196597,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.30349144,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.55037183,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.35520327,\n",
      "       -0.45976067, -4.1984973 ]), done=False), Experience(state=array([0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       6.85092032e-01, 0.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
      "       0.00000000e+00, 6.07429862e-01, 1.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 4.53720242e-01, 0.00000000e+00,\n",
      "       1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 8.90898347e-01,\n",
      "       1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       3.65091294e-01, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 3.07766855e-01, 0.00000000e+00, 0.00000000e+00,\n",
      "       1.00000000e+00, 0.00000000e+00, 3.60183746e-01, 4.10601497e-05,\n",
      "       3.47636199e+00]), action=2, reward=0.0, next_state=array([0.        , 1.        , 0.        , 0.        , 0.97346324,\n",
      "       0.        , 0.        , 0.        , 1.        , 0.        ,\n",
      "       0.        , 0.        , 1.        , 0.        , 0.87086731,\n",
      "       1.        , 0.        , 0.        , 0.        , 0.30276769,\n",
      "       1.        , 0.        , 0.        , 0.        , 0.39464349,\n",
      "       0.        , 0.        , 0.        , 1.        , 0.        ,\n",
      "       1.        , 0.        , 0.        , 0.        , 0.40990984,\n",
      "       1.08183753, 1.87375045]), done=False), Experience(state=array([ 1.        ,  0.        ,  0.        ,  0.        ,  0.15498984,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.10181672,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.43432799,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.49712312,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.43767783,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.38658658,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.08328444,\n",
      "        2.76584196, -3.66645765]), action=2, reward=0.0, next_state=array([ 0.        ,  1.        ,  0.        ,  0.        ,  0.49420136,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.08882746,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.35030937,\n",
      "        0.        ,  1.        ,  0.        ,  0.        ,  0.52391696,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.16068323,\n",
      "        1.        ,  0.        ,  0.        ,  0.        ,  0.1004708 ,\n",
      "        0.        ,  0.        ,  1.        ,  0.        ,  0.71067291,\n",
      "        0.53451693, -4.33477068]), done=False), Experience(state=array([0.        , 1.        , 0.        , 0.        , 0.26184216,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.15548445,\n",
      "       0.        , 0.        , 0.        , 1.        , 0.        ,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.17342041,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.30037278,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.15076014,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.1833438 ,\n",
      "       0.52364457, 7.90705538]), action=2, reward=0.0, next_state=array([0.        , 1.        , 0.        , 0.        , 0.1556771 ,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.1995331 ,\n",
      "       0.        , 0.        , 0.        , 1.        , 0.        ,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.1410913 ,\n",
      "       0.        , 0.        , 1.        , 0.        , 0.89404601,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.15567699,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.33385   ,\n",
      "       1.99193382, 2.97676826]), done=False)]\n",
      "Experience(state=array([0.        , 1.        , 0.        , 0.        , 0.26184216,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.15548445,\n",
      "       0.        , 0.        , 0.        , 1.        , 0.        ,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.17342041,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.30037278,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.15076014,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.1833438 ,\n",
      "       0.52364457, 7.90705538]), action=2, reward=0.0, next_state=array([0.        , 1.        , 0.        , 0.        , 0.1556771 ,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.1995331 ,\n",
      "       0.        , 0.        , 0.        , 1.        , 0.        ,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.1410913 ,\n",
      "       0.        , 0.        , 1.        , 0.        , 0.89404601,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.15567699,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.33385   ,\n",
      "       1.99193382, 2.97676826]), done=False)\n",
      "64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shashank/deep-reinforcement-learning/p1_navigation/dqn_agent.py:195: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  is_weights[i,0] =  ((BATCH_SIZE * sample_probability)**(-self.b)) / max_weight\n",
      "/home/shashank/deep-reinforcement-learning/p1_navigation/dqn_agent.py:195: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  is_weights[i,0] =  ((BATCH_SIZE * sample_probability)**(-self.b)) / max_weight\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'action'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-43ae2b64ef54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# plot the scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-43ae2b64ef54>\u001b[0m in \u001b[0;36mdqn\u001b[0;34m(n_episodes, max_t, eps_start, eps_end, eps_decay)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_done\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deep-reinforcement-learning/p1_navigation/dqn_agent.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0;31m#print(\"---Learning---\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                 \u001b[0msample_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m                 \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0mis_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deep-reinforcement-learning/p1_navigation/dqn_agent.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mnext_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_state\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deep-reinforcement-learning/p1_navigation/dqn_agent.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mnext_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_state\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'action'"
     ]
    }
   ],
   "source": [
    "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        #state = env.reset()\n",
    "        env_info = env.reset(train_mode = True)[brain_name]\n",
    "        state = env_info.vector_observations[0]\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            #next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            env_info = env.step(action)[brain_name]\n",
    "            next_state = env_info.vector_observations[0]\n",
    "            reward = env_info.rewards[0]\n",
    "            done = env_info.local_done[0]\n",
    "            \n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "       \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window)>=200.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "scores = dqn()\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
